CLAUDE_API_KEY: 
GEMINI_API_KEY: 
GROQ_API_KEY: 
OPENAI_API_KEY: 
MISTRAL_API_KEY: 
SERPER_API_KEY: 
LLM_SHERPA_SERVER: http://localhost:5010/api/parseDocument?renderFormat=all&useNewIndentParser=yes

# LLM Server to use 
# Possible values : claude, mistral, openai, ollama, groq, vllm
LLM_SERVER: 

# For Docker, use http://host.docker.internal:11434
# For local, use http://localhost:11434
OLLAMA_HOST: 
# Ollama Model name used by the Ollama server. Will be automatically pulled if not present.
# Example : phi3:instruct
OLLAMA_MODEL: 
